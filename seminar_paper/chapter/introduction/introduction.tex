% !TEX root = ../../seminar.tex


\section{Introduction}
\label{sec:introduction}

It was observed\todo{rephrase} that most software build in a bachelor's or master's thesis is poorly evaluated or not not evaluated at all. Therefore our initial aim was finding or creating a method to evaluate and compare software suited for students.

But we soon discovered a major problem: Large pieces of software are hard to evaluate because effects can be hard to isolate.
To find out which software \todo{SW architecture, devel method..} is more suitable for a given problem, measurements are needed for a quantitative comparison. Moreover, a controlled study is needed to attain valid measurements that allow comparison. Students often struggle with obtaining this \emph{empirical evidence} (see table \ref{table:issuesEBSE}) leading to unusable results making a proper comparison difficult or not possible at all. 

So we revised the question to: How to create a supporting system for students helping to improve the quality (in particular the substantiality) of their studies in the domain of software engineering.

The final system we intend is an electronical\todo{digital?} database containing a collection of experiments. The system is meant to simplify searching, scanning and comparison of experiments. Allowing to quickly find existing evidence that can be used for software design decisions. To populate that collection, a process to create evidence correctly and a compact representation of the results are needed. We propose two documents: \emph{\checklist} to guide students through the scientific process and \emph{\briefingform} for a compact, structured and complete resume of the conducted experiment. 

\todo{rest of chapter are notes}

\begin{itemize}
\item observation: most software build in a bachelor or master thesis is poorly evaluated (if it is done at all)
\item Initial aim: Find/create a method to evaluate and compare software developed by students.
\item But: 1. Large pieces of software are hard to evaluate because effects can be hard to isolate.
\item 2.  To find out wich software is more suitable for a given problem, measurements are needed for a quantitative comparison that can be treated as empirical evidence.
\item To attain valid measurements that allow comparison, a controlled study is needed.
Students sometimes struggle with making that kind of controlled study \todo{tabellenverweis table \ref{table:issuesEBSE}} leading to unusable results making a proper comparison difficult or not possible at all.
\item So we revised the question to:
How to create a supporting system for students helping to improve the quality (in particular the substantiality) of their studies in the domain of software engineering.

% \item > Evidence based software engineering 
% \item aim: aid students in conducting a similar approach. Tools: guidelines, checklists
\end{itemize}

The final system we intend is an electronical database containing a collection of experiments. The system is meant to simplify searching, scanning and comparison of experiments. Allowing to quickly find existing evidence that can be used for software design decisions. To populate that collection, a process to create evidence correctly and a compact representation of the results are needed. We propose two documents: \emph{\checklist} to guide students through the scientific process and \emph{\briefingform} for a compact, structured and complete resume of the conducted experiment. 



\begin{itemize}
\item experimentation/empiricism in CS?
\item \q{should computer scientists experiment more?, Tichy 1997}
\item Experimental evaluation in CS: A quantitative study, 1994: 40\% of papers requiring quant evaluation have none (other disciplins: 15\%)
\item \q{A survey of Controlled Experiments in SE, 2005} from 1993 to 2002: 1,9\% of software engineering paper report controlled experiments
\item \q{Belief \& Evidence in Empirical Software Engineering}, 2016: programmers have strong beliefs, primarily formed by personal opinion (research papers just above "other", below peer opinion, mentor opinion and trade journals); beliefs vary between projects, but do not necessarily correspond with evidence in that project
\item \q{Incorrect results in software engineering experiments: How to improve research practices}, 2016: research and publication bias quite common, need to improve research practices (a key: avoid experiments with low statistical power)
\end{itemize}


















